# AI音声入力ツール — 要件定義書

> **バージョン**: 1.1
> **最終更新**: 2025-02-19
> **対象環境**: Windows 10/11 / ローカル実行専用 / RTX 5070 Ti (16GB VRAM)

---

## 1. プロジェクト概要

### 1.1 目的

開発作業中の音声入力を効率化するデスクトップ常駐ツールを個人開発する。
Push-to-Talk方式でマイク入力を受け付け、ローカルのSTT（Speech-to-Text）で文字起こしした後、
ローカルLLMで積極的にテキストを整形し、アクティブなテキストフィールドに自動挿入する。

### 1.2 スコープ

| 範囲内 | 範囲外 |
|--------|--------|
| Windows デスクトップアプリ | macOS / Linux / モバイル |
| ローカル処理のみ（GPU推論） | クラウドAPI連携 |
| 個人利用 | マルチユーザー / 外部配布 |
| 日本語＋英語技術用語の混在入力 | 他言語対応 |
| AIチャット・ターミナルへの入力 | メール・ビジネス文書向け整形 |

### 1.3 ユーザー像

- 開発者（1名、個人利用）
- ヘビーユーザー（1日50回以上使用）
- 主な使用場面: AIチャット（ChatGPT、Claude等）への入力、ターミナルへのコマンド入力
- デスクトップコンデンサーマイクを使用、静かな自室環境

---

## 2. 機能要件

### FR-01: Push-to-Talk 録音

| 項目 | 仕様 |
|------|------|
| トリガーキー | 右Altキー（設定で変更可能） |
| 録音モード | ホールドモードのみ（キーを押している間だけ録音、離すと確定） |
| トグル/ロックモード | 不要 |
| キャンセル機能 | 不要 |
| 最大録音時間 | 60秒（超過時は自動的に録音停止し、それまでの音声を処理する） |
| 連続入力 | 非対応（前回の処理完了を待ってから次の録音を受け付ける） |
| 音声フォーマット | 16kHz / 16bit / モノラル WAV |

**補足: 連続入力の排他制御**

処理中に再度Push-to-Talkキーが押された場合の動作:
- 処理中であることを視覚フィードバックで示す
- 押下は無視し、処理完了後に再度受け付け可能にする

### FR-02: 音声認識（STT）

STTエンジンは設定ファイルで切り替え可能とする。デフォルトは faster-whisper。

**エンジン比較:**

| 項目 | faster-whisper (デフォルト) | SenseVoice-Small |
|------|--------------------------|-----------------|
| モデル | `large-v3-turbo` | `SenseVoiceSmall` |
| 推論時間 (15s音声) | ~1.5-2.0s | ~0.1s |
| VRAM | ~4GB (float16) | ~2GB |
| 日本語精度 | ○ (実績あり) | ◎ (アジア言語に強い) |
| 英語混在 | ○ | ○ (50+言語対応) |
| VAD | 内蔵 Silero VAD | 内蔵 |
| 備考 | 安定・実績重視 | 速度重視・要検証 |

**共通仕様:**

| 項目 | 仕様 |
|------|------|
| GPU推論 | CUDA（RTX 5070 Ti） |
| モデル常駐 | する（起動時にロードし、アプリ終了までVRAMに保持） |

**設定ファイル（config.yaml）:**

```yaml
stt:
  engine: "faster-whisper"  # or "sensevoice"
  faster_whisper:
    model: "large-v3-turbo"
    device: "cuda"
    compute_type: "float16"
    language: "ja"
    vad:
      min_speech_duration_ms: 250
      min_silence_duration_ms: 500
  sensevoice:
    model: "FunAudioLLM/SenseVoiceSmall"
    device: "cuda"
    language: "auto"  # 自動言語検出対応
```

**設計方針:** STTエンジンを抽象化するインターフェース（`STTEngine` ABC）を定義し、実装を差し替え可能にする。詳細は「4.5 STTエンジン抽象化」を参照。

**VADパラメータ（faster-whisper使用時）:**

| パラメータ | 値 | 説明 |
|-----------|-----|------|
| `min_speech_duration_ms` | 250 | この長さ未満の音声は無視 |
| `min_silence_duration_ms` | 500 | 発話区間の区切り判定 |

### FR-03: LLM テキスト整形

| 項目 | 仕様 |
|------|------|
| 推論バックエンド | Ollama（推奨）またはLM Studio（OpenAI互換API） |
| モデル | `qwen2.5-7b-instruct`（Q4_K_M量子化、デフォルト） |
| 代替モデル | `qwen2.5-3b-instruct`（Q4_K_M量子化、軽量構成用） |
| 整形レベル | 積極的（Typeless風: 文章構造の再構成を含む） |
| コンテキスト認識 | 不要（常に同じ文体） |
| LLMスキップモード | 不要（常にLLMを通す） |
| temperature | 0.3 |
| max_tokens | 1024 |

**モデルサイズ比較:**

| モデル | VRAM | 推論速度 | 整形品質 |
|--------|------|---------|---------|
| Qwen2.5-7B (Q4_K_M) | ~6GB | ~1-3s | ◎ |
| Qwen2.5-3B (Q4_K_M) | ~3GB | ~0.5-1.5s | ○ |

設定ファイルでモデルを切り替える:

```yaml
llm:
  backend: "ollama"
  model: "qwen2.5:7b-instruct-q4_K_M"  # or "qwen2.5:3b-instruct-q4_K_M"
  base_url: "http://localhost:11434/v1"
  temperature: 0.3
  max_tokens: 1024
```

**整形ルール（LLMへのシステムプロンプトで制御）:**

1. フィラー（「えーと」「あのー」「まあ」「えー」「うーん」等）を完全に除去する
2. 言い間違い・繰り返し・言い直しを修正する
3. しどろもどろな発話から意図を汲み取り、明確な文章に再構成する
4. 適切な句読点（。、）を追加する
5. 文法的な誤りを修正する
6. 技術用語・英語用語は正しい表記にする
7. 元の意味・意図は変えない。情報を追加しない
8. 整形済みテキストのみを出力する（説明や注釈は不要）

### FR-04: テキスト挿入

| 項目 | 仕様 |
|------|------|
| 挿入方式 | クリップボード経由（pyperclip + Ctrl+V） |
| 挿入先 | アクティブなテキストフィールドに自動挿入 |
| 元のクリップボード | 挿入後に復元する |
| 挿入前遅延 | 50ms（対象アプリのフォーカス安定のため） |
| Undo対応 | 不要（アプリ側のCtrl+Zに委ねる） |

### FR-05: 処理中フィードバック（UI）

| 状態 | フィードバック |
|------|--------------|
| 待機中 | システムトレイにアイコン表示（通常色） |
| 録音中 | 小さなポップアップ/オーバーレイ表示（🎙️ 録音中...） |
| 処理中（STT） | ポップアップ表示更新（📝 文字起こし中...） |
| 処理中（LLM） | ポップアップ表示更新（🔧 整形中...） |
| 完了 | ポップアップを短時間表示後に自動消去（✅ 完了） |
| エラー | ポップアップにエラー内容を表示（❌ エラー: ...） |

**ポップアップの仕様:**

- 画面右下に小さく表示（他のアプリの操作を邪魔しない）
- 常に最前面に表示
- フォーカスを奪わない（アクティブウィンドウを変えない）
- 完了後は1〜2秒で自動消去

### FR-06: 自動起動

- Windowsスタートアップに登録し、ログイン時に自動起動する
- 起動時にSTT/LLMモデルをバックグラウンドでプリロードする
- モデルロード完了まではPush-to-Talkを無効化し、トレイアイコンで「準備中」を示す

---

## 3. 非機能要件

### NFR-01: パフォーマンス

| 指標 | 目標値 | 備考 |
|------|--------|------|
| 待機時CPU使用率 | 実質 0%（< 0.5%） | ホットキーリスナーのみ稼働 |
| 待機時メモリ | 200MB以下（Python本体 + ホットキーリスナー） | モデルのVRAMは別計上 |
| 待機時VRAM | 約5〜10GB（構成による、詳細はNFR-02参照） | RTX 5070 Ti 16GBの31〜63% |
| 録音→テキスト挿入（E2E） | 5秒以内（〜15秒の発話） | デフォルト構成、STT + LLM合計 |
| 録音→テキスト挿入（E2E） | 2秒以内（〜15秒の発話） | 最速構成（SenseVoice + 3B） |
| 録音→テキスト挿入（E2E） | 10秒以内（〜60秒の発話） | 長文許容 |
| 初回起動時間 | 20秒以内 | モデルロード含む |

**レイテンシ内訳（目安）:**

```
■ faster-whisper + 7B LLM 構成 (デフォルト)
録音停止 → VADトリミング:     ~50ms
VAD → STT処理:              1.0 〜 2.0s
STT → LLM整形:              1.0 〜 3.0s
LLM → テキスト挿入:          ~100ms
合計:                        2.0 〜 5.0s

■ SenseVoice + 7B LLM 構成 (速度重視)
録音停止 → VADトリミング:     ~50ms
VAD → STT処理:              0.1 〜 0.3s
STT → LLM整形:              1.0 〜 3.0s
LLM → テキスト挿入:          ~100ms
合計:                        1.3 〜 3.5s

■ SenseVoice + 3B LLM 構成 (最速)
録音停止 → VADトリミング:     ~50ms
VAD → STT処理:              0.1 〜 0.3s
STT → LLM整形:              0.5 〜 1.5s
LLM → テキスト挿入:          ~100ms
合計:                        0.8 〜 2.0s
```

### NFR-02: VRAM管理

**構成別VRAM消費:**

| 構成 | STT | LLM | 合計 | 使用率 (16GB) |
|------|-----|-----|------|--------------|
| faster-whisper + 7B (デフォルト) | ~4GB | ~6GB | ~10GB | 63% |
| faster-whisper + 3B | ~4GB | ~3GB | ~7GB | 44% |
| SenseVoice + 7B | ~2GB | ~6GB | ~8GB | 50% |
| SenseVoice + 3B (最軽量) | ~2GB | ~3GB | ~5GB | 31% |

| 項目 | 仕様 |
|------|------|
| 他GPUアプリとの共存 | 考慮不要（開発中は本ツールのみ） |

**GPUメモリ不足時のフォールバック:**

RTX 5070 TiのVRAMが不足する状況（ドライバ更新やシステム変更後等）に備え、
以下の段階的な縮退を行う:

1. LLMをより小さいモデルに切り替え（`qwen2.5-3b` → 約3GB）
2. STTをSenseVoiceに切り替え（約2GB）
3. 上記でも不足する場合はエラー通知して停止

切り替えは手動（config.yaml編集）で行う。自動切り替えは初期バージョンでは不要。

### NFR-03: 安定性

| 項目 | 仕様 |
|------|------|
| 長時間稼働 | メモリリークなく24時間以上動作すること |
| エラー回復 | STT/LLMの推論エラー時、次回の録音は正常に受け付けること |
| GPUリセット | CUDAエラー発生時、自動でモデルを再ロードすること |

### NFR-04: プライバシー・データ管理

| 項目 | 仕様 |
|------|------|
| 音声データ | 処理後即削除（WAVファイルをディスクに残さない） |
| テキスト履歴 | 保存しない |
| ログ | デバッグ用のコンソール出力のみ（ファイルに永続化しない） |
| ネットワーク通信 | 一切なし（完全オフライン動作） |

### NFR-05: 処理タイムアウト

Typelessの仕様を参考に、以下のタイムアウト設計とする。
Typelessでは録音の上限を6分に設定し、5分時点で警告カウントダウンを表示する方式を採っている。

本ツールでは以下の方針とする:

| 処理 | タイムアウト | タイムアウト時の動作 |
|------|------------|-------------------|
| 録音 | 60秒 | 自動停止し、それまでの音声を処理 |
| STT推論 | 30秒 | エラー通知。テキスト挿入なし |
| LLM推論 | 30秒 | STTの生テキストをそのまま挿入 |

**LLMタイムアウト時のフォールバック:**

LLMの処理が30秒を超えた場合、LLM処理を中断し、
STTで得られた生テキストをそのまま挿入する。
これにより「何も得られない」状況を回避する。

---

## 4. 技術構成

### 4.1 VRAM配分図

**デフォルト構成（faster-whisper + 7B）:**

```
RTX 5070 Ti - 16GB VRAM
┌──────────────────────────────────────────────────┐
│  Whisper large-v3-turbo (~4GB)  │  Qwen2.5-7B Q4 (~6GB)  │  空き (~6GB)  │
│  ████████████████████████████    │  ██████████████████████  │               │
│  STT (常駐)                     │  LLM (常駐)             │  OS/CUDA予約  │
└──────────────────────────────────────────────────┘
```

**最軽量構成（SenseVoice + 3B）:**

```
RTX 5070 Ti - 16GB VRAM
┌──────────────────────────────────────────────────┐
│  SenseVoice (~2GB)  │  Qwen2.5-3B Q4 (~3GB)  │  空き (~11GB)          │
│  ██████████████      │  ████████████████████    │                       │
│  STT (常駐)         │  LLM (常駐)             │  OS/CUDA予約 + 余裕    │
└──────────────────────────────────────────────────┘
```

### 4.2 処理パイプライン

```
[右Alt押下]
    │
    ▼
┌─────────────────┐
│  マイク録音開始    │  ← ポップアップ: 🎙️ 録音中...
│  (sounddevice)   │
└────────┬────────┘
    [右Alt解放]
    │
    ▼
┌─────────────────────────┐
│  STT推論 (STTEngine ABC) │  ← ポップアップ: 📝 文字起こし中...
│  ┌───────────────────┐   │     タイムアウト: 30s
│  │ faster-whisper     │   │     ※ エンジン内でVAD処理も実行
│  │ or SenseVoice      │   │
│  └───────────────────┘   │
└────────┬────────────────┘
    │ 生テキスト
    ▼
┌─────────────────┐
│  LLM整形          │  ← ポップアップ: 🔧 整形中...
│  (Ollama /        │     タイムアウト: 30s
│   LM Studio)      │     フォールバック: 生テキスト挿入
└────────┬────────┘
    │ 整形済みテキスト
    ▼
┌─────────────────┐
│  テキスト挿入      │  ← ポップアップ: ✅ 完了（1-2秒で消去）
│  (clipboard +     │
│   Ctrl+V)         │
└─────────────────┘
```

### 4.3 ソフトウェア構成

| レイヤー | ライブラリ / ツール | 役割 |
|---------|-------------------|------|
| ホットキー | `pynput` | 右Altキーの押下/解放検出 |
| オーディオ | `sounddevice` + `numpy` | マイク入力キャプチャ |
| STT (選択肢1) | `faster-whisper` | ローカル音声認識 (デフォルト) |
| STT (選択肢2) | `funasr` (SenseVoice) | ローカル音声認識 (高速) |
| VAD | Silero VAD (faster-whisper内蔵) / SenseVoice内蔵 | 音声区間検出 |
| LLM | Ollama Python SDK (OpenAI互換) | テキスト整形 |
| テキスト挿入 | `pyperclip` + Win32 API | クリップボード経由貼り付け |
| UI | `tkinter`（オーバーレイ） + `pystray`（トレイ） | フィードバック表示 |
| 設定 | `pydantic` + YAML | 設定管理 |

### 4.4 LLM バックエンド互換

Ollama と LM Studio の両方に対応する。
いずれもOpenAI互換APIを提供するため、`base_url` の切り替えのみで対応可能。

| バックエンド | エンドポイント | 備考 |
|-------------|--------------|------|
| Ollama | `http://localhost:11434/v1` | `ollama serve` で起動 |
| LM Studio | `http://localhost:1234/v1` | GUIからサーバー起動 |

設定ファイルでバックエンドを切り替える:

```yaml
llm:
  backend: "ollama"  # or "lmstudio"
  ollama:
    base_url: "http://localhost:11434/v1"
    model: "qwen2.5:7b-instruct-q4_K_M"
  lmstudio:
    base_url: "http://localhost:1234/v1"
    model: "qwen2.5-7b-instruct"
```

### 4.5 STTエンジン抽象化

STTエンジンを差し替え可能にするため、抽象基底クラス（ABC）を定義する。

```python
from abc import ABC, abstractmethod
import numpy as np

class STTEngine(ABC):
    @abstractmethod
    def load_model(self) -> None: ...

    @abstractmethod
    def transcribe(self, audio: np.ndarray, sample_rate: int) -> str: ...

    @abstractmethod
    def get_vram_usage_mb(self) -> int: ...

class FasterWhisperEngine(STTEngine): ...
class SenseVoiceEngine(STTEngine): ...
```

各エンジンはVAD処理を内部で行い、`transcribe()` メソッドに音声データを渡すだけで文字起こし結果を返す。

---

## 5. エラーハンドリング

| エラー条件 | 対応 |
|-----------|------|
| STTが空テキストを返した | 何もしない（ポップアップを消去して待機状態に戻る） |
| LLMがハルシネーション | そのまま挿入（ユーザーが手動修正） |
| LLMタイムアウト（30秒超過） | STTの生テキストをフォールバック挿入 |
| STTタイムアウト（30秒超過） | エラー通知、テキスト挿入なし |
| CUDAエラー（GPU異常） | モデルを再ロードして復旧を試行。失敗時はエラー通知 |
| マイクが見つからない | 起動時にエラー通知、デバイス選択を促す |
| Ollamaサーバー未起動 | 起動時にエラー通知。接続リトライ（3回） |
| 処理中にPush-to-Talk押下 | 無視（処理完了後に受け付け再開） |

---

## 6. テスト戦略

### 6.1 STT精度の評価

**方式: 主観評価 + 定番フレーズチェック**

感覚的に「十分使える」ことを基準とするが、
以下の定番フレーズで最低限の品質を担保する:

```
テストフレーズ集（手動で録音し、出力を確認）:
1. 「Dockerコンテナをビルドして、Kubernetesにデプロイしたい」
2. 「TypeScriptでReactコンポーネントを作成してください」
3. 「git checkout -b feature/user-authentication」
4. 「えーと、あの、このAPIのレスポンスがnullになるんですけど、
    あ、いや、undefinedか。えーっと、どうすればいいですか」
5. 「Next.jsのApp Routerでミドルウェアを設定する方法を教えて」
```

**合格基準:**
- テストフレーズ1〜3: 技術用語が正しく認識される
- テストフレーズ4: LLM整形後にフィラーが除去され、意図が明確になる
- テストフレーズ5: 固有名詞（Next.js、App Router）が正しい

### 6.2 LLM整形品質の評価

**方式: 入力→期待出力ペアによる目視比較**

以下のテストケースを用意し、LLMの出力を確認する:

| # | 生テキスト（STT出力を模擬） | 期待する整形結果 | 確認ポイント |
|---|---------------------------|----------------|-------------|
| 1 | えーとDockerコンテナをビルドしてKubernetesにデプロイしたいんですけど | DockerコンテナをビルドしてKubernetesにデプロイしたい。 | フィラー除去、句読点 |
| 2 | あのーこのバグの原因なんですけどまあ多分あのーNullPointerだと思うんですよねいやNullReferenceかな | このバグの原因は、おそらくNullReferenceだと思います。 | 言い直し修正、再構成 |
| 3 | lsマイナスlaスラッシュvar/log | `ls -la /var/log` | ターミナルコマンド認識 |
| 4 | 昨日のミーティングで田中さんが言ってたやつあれってなんだっけ | 昨日のミーティングで田中さんが言っていた件は何でしたか？ | 口語→文語変換 |

**確認頻度:** モデル変更時、プロンプト変更時に実施

### 6.3 回帰テスト

**方式: 簡易スクリプトによる自動テスト**

モデルやプロンプトを変更した際の品質劣化を検出するため、
以下の仕組みを用意する:

1. テスト用の音声ファイル（WAV）を5〜10個用意する
2. 各音声に対する「最低限含まれるべきキーワード」を定義する
3. スクリプトで一括処理し、キーワードの有無をチェックする

```python
# test_regression.py（イメージ）
test_cases = [
    {
        "audio": "test_audio/docker_deploy.wav",
        "must_contain": ["Docker", "ビルド", "Kubernetes", "デプロイ"],
        "must_not_contain": ["えーと", "あのー"],
    },
    ...
]
```

**実施タイミング:**
- Whisperモデルの変更時
- LLMモデルの変更時
- システムプロンプトの変更時

---

## 7. 開発フェーズ

### Phase 1: MVP（最小実行可能プロダクト）

**目標: 「右Altを押して話す → 整形されたテキストが挿入される」を動かす**

1. Push-to-Talk録音（pynput + sounddevice）
2. STTエンジン抽象化 + faster-whisper 実装
3. ローカルLLM整形（Ollama, qwen2.5-7b）
4. テキスト挿入（clipboard + Ctrl+V）
5. コンソールへのログ出力（フィードバック代替）

### Phase 2: UX改善 + 高速化

6. ポップアップ/オーバーレイによる視覚フィードバック
7. システムトレイアイコン
8. タイムアウト処理・エラーハンドリング
9. VRAM不足時のフォールバック
10. SenseVoice-Small 対応追加
11. Deep Context 検討・実装
    - Windows UI Automation API でアクティブウィンドウのテキスト取得
    - 取得テキストをLLMプロンプトにコンテキストとして注入

### Phase 3: 安定化・自動化

12. Windows自動起動（スタートアップ登録）
13. 回帰テストスクリプト
14. LM Studio互換対応
15. 長時間稼働の安定性検証

---

## 8. 品質指標の優先順位

1. **操作の快適さ（UX）** — ストレスなく使えること
2. **応答速度（レイテンシ）** — 5秒以内を目標
3. **認識精度** — 技術用語が正しく認識されること
4. **VRAM消費の少なさ** — 構成に応じて5〜10GBに収めること
5. **拡張性・保守性** — 個人ツールとして最低限のコード品質

---

## 9. 前提条件・制約

| 項目 | 内容 |
|------|------|
| OS | Windows 10 / 11 |
| GPU | NVIDIA RTX 5070 Ti (16GB VRAM) |
| CUDA | Toolkit 12.x + cuDNN |
| Python | 3.11以上 |
| マイク | デスクトップコンデンサーマイク |
| ネットワーク | 不要（完全オフライン動作） |
| LLMサーバー | Ollama または LM Studio がインストール・起動済みであること |
| ストレージ | モデルファイル用に約15GB（Whisper ~3GB + LLM ~10GB） |

---

## 10. 用語集

| 用語 | 説明 |
|------|------|
| STT | Speech-to-Text。音声をテキストに変換する処理 |
| VAD | Voice Activity Detection。音声区間検出 |
| Push-to-Talk | キーを押している間だけ録音する方式 |
| フィラー | 「えーと」「あのー」等の無意味な発話 |
| VRAM | GPU搭載のビデオメモリ |
| faster-whisper | OpenAI Whisperの高速化実装（CTranslate2ベース） |
| SenseVoice | Alibaba FunAudioLLMによる高速音声認識モデル。アジア言語に強い |
| FunASR | SenseVoiceモデルを利用するためのPythonライブラリ |
| Ollama | ローカルLLM実行環境 |
| LM Studio | GUIベースのローカルLLM実行環境 |
| ハルシネーション | LLMが原文にない内容を勝手に生成すること |
| Q4量子化 | モデルの重みを4ビットに圧縮する技術。VRAM削減・速度向上 |
